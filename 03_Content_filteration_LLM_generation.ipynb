{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Filtering and Getting Matched Context\n",
    "\n",
    "```markdown\n",
    "# Filtering and Getting Matched Context\n",
    "\n",
    "This notebook demonstrates how to filter and get matched context based on a question from the tokenized sentences obtained from a PDF document.\n",
    "\n",
    "## Step 1: Install Required Libraries\n",
    "\n",
    "First, we need to install the necessary libraries. Run the following command in your terminal or in a Jupyter Notebook cell:\n",
    "\n",
    "```python\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Import Libraries\n",
    "\n",
    "Next, we will import the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\shuklajiwank\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\shuklajiwank\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import json\n",
    "\n",
    "# Ensure punkt tokenizer is downloaded\n",
    "# Install PyMuPDF if you haven't already\n",
    "# !pip install pymupdf\n",
    "\n",
    "\n",
    "import re\n",
    "from typing import List, Dict\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Load Tokenized Sentences\n",
    "\n",
    "We assume you have already extracted and tokenized the sentences using the previous notebook. Here, we will load those tokenized sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"page_number\": 1,\n",
      "        \"chunks\": [\n",
      "            {\n",
      "                \"paragraph\": 1,\n",
      "                \"text\": \"This is an initiative aiming to combat misinformation in the age of LLMs (Correspondence to: Kai Shu) (New Preprint) Can Knowledge Editing Really Correct Hallucinations? - We proposed HalluEditBench to holistically benchmark knowledge editing methods in correcting real-world hallucinations on five dimensions including Efficacy, Generalization, Portability, Locality, and Robustness. We find their effectiveness could be far from what their performance on existing datasets suggests, and the performance beyond Efficacy for all methods is generally unsatisfactory. (New Preprint) Can Editing LLMs Inject Harm? - We propose to reformulate knowledge editing as a new type of safety threat for LLMs, namely Editing Attack, and discover its emerging risk of injecting misinformation or bias into LLMs stealthily, indicating the feasibility of disseminating misinformation or bias with LLMs as new channels. (SIGKDD Explorations 2024) Authorship Attribution in the Era of LLMs: Problems, Methodologies, and Challenges - This survey paper systematically categorizes authorship attribution in the era of LLMs into four problems: attributing unknown texts to human authors, detecting LLM-generated texts, identifying specific LLMs or human authors, and classifying texts as human-authored, machine-generated, or co-authored by both, while also highlighting key challenges and open problems. (EMNLP 2024 Findings) Can Large Language Models Identify Authorship? -\"\n",
      "            },\n",
      "            {\n",
      "                \"paragraph\": 2,\n",
      "                \"text\": \"We propose Linguistically Informed Prompting (LIP) strategy, which offers in-context linguistic guidance, to boost LLMs' reasoning capacity for authorship verification and attribution tasks, while also providing natural language explanations. (AI Magazine 2024) Combating Misinformation in the Age of LLMs: Opportunities and Challenges - A survey of the opportunities (can we utilize LLMs to combat misinformation) and challenges (how to combat LLM-generated misinformation) of combating misinformation in the age of LLMs. (Proceedings of ICLR 2024) Can LLM-Generated Misinformation Be 12/23/24, 2:22 PM LLMs Meet Misinformation https://llm-misinformation.github.io 1/15\"\n",
      "            }\n",
      "        ]\n",
      "    },\n",
      "    {\n",
      "        \"page_number\": 2,\n",
      "        \"chunks\": [\n",
      "            {\n",
      "                \"paragraph\": 1,\n",
      "                \"text\": \"Detected? - We discover that LLM-generated misinformation can be harder to detect for humans and detectors compared to human-written misinformation with the same semantics, which suggests it can have more deceptive styles and potentially cause more harm. Combating Misinformation in the Age of LLMs: Opportunities and Challenges Canyu Chen, Kai Shu Illinois Institute of Technology Published at AI Magazine 2024 (Volume 45, Issue 3, Fall 2024), Highlight Article Publication Paper \\ue974arXiv Talk Slides 1 Slides 2 Paper List 12/23/24, 2:22 PM LLMs Meet Misinformation https://llm-misinformation.github.io 2/15\"\n",
      "            }\n",
      "        ]\n",
      "    },\n",
      "    {\n",
      "        \"page_number\": 3,\n",
      "        \"chunks\": [\n",
      "            {\n",
      "                \"paragraph\": 1,\n",
      "                \"text\": \"Abstract Misinformation such as fake news and rumors is a serious threat to information ecosystems and public trust. The emergence of Large Language Models (LLMs) has great potential to reshape the landscape of combating misinformation. Generally, LLMs can be a double-edged sword in the fight. On the one hand, LLMs bring promising opportunities for combating misinformation due to their profound world knowledge and strong reasoning abilities. Thus, one emergent question is: can we utilize LLMs to combat misinformation? On the other hand, the critical challenge is that LLMs can be easily leveraged to generate deceptive misinformation at scale. Then, another important question is: how to combat LLM-generated misinformation? In this paper, we first systematically review the history of combating misinformation before the advent of LLMs. Then we illustrate the current efforts and present an outlook for these two fundamental questions respectively. The goal of this survey paper is to facilitate the progress of utilizing LLMs for fighting misinformation and call for interdisciplinary efforts from different stakeholders for combating LLM-generated misinformation. BibTeX @article{chen2024combatingmisinformation, author = {Chen, Canyu and Shu, Kai}, title = {Combating misinformation in the age of LLMs: Opportunities and cha journal = {AI Magazine}, year = {2024}, doi =\"\n",
      "            },\n",
      "            {\n",
      "                \"paragraph\": 2,\n",
      "                \"text\": \"{10.1002/aaai.12188}, url = {https://doi.org/10.1002/aaai.12188} } 12/23/24, 2:22 PM LLMs Meet Misinformation https://llm-misinformation.github.io 3/15\"\n",
      "            }\n",
      "        ]\n",
      "    },\n",
      "    {\n",
      "        \"page_number\": 4,\n",
      "        \"chunks\": [\n",
      "            {\n",
      "                \"paragraph\": 1,\n",
      "                \"text\": \"Can LLM-Generated Misinformation Be Detected? Canyu Chen, Kai Shu Illinois Institute of Technology Published at Proceedings of ICLR 2024 Publication Paper \\ue974arXiv Dataset and Code Talk Slides 1 Slides 2 post post post Abstract The advent of Large Language Models (LLMs) has made a transformative impact. However, the potential that LLMs such as ChatGPT can be exploited to generate misinformation has posed a serious concern to online safety and public trust. A fundamental research question is: will LLM-generated misinformation cause more harm than human-written misinformation? We propose to tackle this question from the perspective of detection difficulty. We first build a taxonomy of LLM-generated misinformation. Then we categorize and validate the potential real-world methods for generating 12/23/24, 2:22 PM LLMs Meet Misinformation https://llm-misinformation.github.io 4/15\"\n",
      "            }\n",
      "        ]\n",
      "    },\n",
      "    {\n",
      "        \"page_number\": 5,\n",
      "        \"chunks\": [\n",
      "            {\n",
      "                \"paragraph\": 1,\n",
      "                \"text\": \"misinformation with LLMs. Then, through extensive empirical investigation, we discover that LLM-generated misinformation can be harder to detect for humans and detectors compared to human- written misinformation with the same semantics, which suggests it can have more deceptive styles and potentially cause more harm. We also discuss the implications of our discovery on combating misinformation in the age of LLMs and the countermeasures. Our Contributions (1) We build a taxonomy by types, domains, sources, intents and errors to systematically characterize LLM-generated misinformation as an emerging and critical research topic. (2) We make the first attempt to categorize and validate the potential real-world methods for generating misinformation with LLMs including Hallucination Generation, Arbitrary Misinformation Generation and Controllable Misinformation Generation methods. (3) We discover that misinformation generated by LLMs can be harder for humans and detectors to detect than human-written misinformation with the same semantic information through extensive investigation, which provides sufficient empirical evidence to demonstrate that LLM-generated misinformation can have more deceptive styles and potentially cause more harm. (4) We discuss the emerging challenges for misinformation detectors (Section 6), important implications of our discovery on combating misinformation in the age of LLMs (Section 7), the countermeasures against LLM-generated misinformation through LLMs\\u2019\"\n",
      "            },\n",
      "            {\n",
      "                \"paragraph\": 2,\n",
      "                \"text\": \"whole lifecycle (Section 8). Taxonomy of LLM-Generated Misinformation We propose to taxonomize LLM-generated misinformation from five dimensions including types, domains, sources, intents and errors. In particular, we categorize the sources of LLM-generated misinformation into hallucination, arbitrary generation and controllable generation since there are different potential methods to generate misinformation with LLMs. Also, we divide the intents of generated misinformation into unintentional and intentional generation considering hallucination can potentially occur in any generation process of LLMs and users without malicious intent may also generate texts containing hallucinated information when using LLMs. 12/23/24, 2:22 PM LLMs Meet Misinformation https://llm-misinformation.github.io 5/15\"\n",
      "            }\n",
      "        ]\n",
      "    },\n",
      "    {\n",
      "        \"page_number\": 6,\n",
      "        \"chunks\": [\n",
      "            {\n",
      "                \"paragraph\": 1,\n",
      "                \"text\": \"RQ1: How Can LLMs be Utilized to Generate Misinformation? We propose to categorize the LLM-based misinformation generation methods into three types based on real-world scenarios (Table 1): Hallucination Generation (HG), Arbitrary Misinformation Generation (AMG) and Controllable Misinformation Generation (CMG). 12/23/24, 2:22 PM LLMs Meet Misinformation https://llm-misinformation.github.io 6/15\"\n",
      "            }\n",
      "        ]\n",
      "    },\n",
      "    {\n",
      "        \"page_number\": 7,\n",
      "        \"chunks\": [\n",
      "            {\n",
      "                \"paragraph\": 1,\n",
      "                \"text\": \"Connection with Jailbreak Attack: Jailbreak attacks usually refer to the attempts to bypass the safety guards of LLMs (e.g., ChatGPT) to generate harmful content. On the one hand, our proposed approaches to generate misinformation with LLMs are motivated by real- world scenarios shown in Table 1 and orthogonal to the previous Jailbreak techniques (Wei et al., 2023a; Zou et al., 2023), which suggests the misinformation generation approaches and previous jailbreak methods could be potentially combined by attackers. On the other hand, the HG methods could be regarded as Unintentional Jailbreak, which is different from most previous jailbreak methods. The AMG and CMG methods could be regarded as Intentional Jailbreak. We test the possibilities that our misinformation generation approaches can bypass the safeguard of ChatGPT by prompting with each approach for 100 times. The Attacking Success Rates are in Table 2. Thus, our first core finding is: LLMs can follow users\\u2019 instructions to generate misinformation in different types, domains, and errors. RQ2: Can Humans Detect LLM-Generated Misinformation? Although previous works have shown that it is hard for humans to detect human-written misinformation (Lyons et al., 2021), it is still under-explored whether or not humans can detect LLM-generated misinformation. Experiment Result Analysis:\"\n",
      "            },\n",
      "            {\n",
      "                \"paragraph\": 2,\n",
      "                \"text\": \"First, we can observe that it is generally hard for humans to detect ChatGPT-generated misinformation, especially those generated with Hallucinated News Generation, Totally Arbitrary Generation, Rewriting Generation, and Open-ended Generation. 12/23/24, 2:22 PM LLMs Meet Misinformation https://llm-misinformation.github.io 7/15\"\n",
      "            }\n",
      "        ]\n",
      "    },\n",
      "    {\n",
      "        \"page_number\": 8,\n",
      "        \"chunks\": [\n",
      "            {\n",
      "                \"paragraph\": 1,\n",
      "                \"text\": \"Second, we attempt to compare the human detection\\u2019s hardness for ChatGPT-generated and human-written misinformation that have the same semantics. We have demonstrated that Paraphrase Generation, Rewriting Generation, and Open-ended Generation generally only change the style information and preserve the original semantics. Comparing human detection performance on human-written misinformation (the grey numbers in Table 3) and ChatGPT-generated misinformation via Paraphrase Generation, Rewriting Generation and Open-ended Generation approaches (the red or green numbers in Table 3), we can discover that the human detection performances on ChatGPT-generated misinformation are mostly lower than those on human-written misinformation. Thus, we can have our second core finding shown as follows: LLM-generated misinformation can be harder for humans to detect than human-written misinformation with the same semantics. Our finding also validates that LLM-generated misinformation can have more deceptive styles for humans and implies humans can be potentially more susceptible to LLM- generated misinformation than human-written misinformation. RQ3: Can Detectors Detect LLM-Generated Misinformation? Emerging Challenges for Detectors: In the real world, detecting LLM-generated misinformation is in face with emerging challenges. First, it is difficult to obtain factuality supervision labels to train detectors for LLM-generated misinformation since it is harder for humans to detect than human-written misinformation. Second, malicious\"\n",
      "            },\n",
      "            {\n",
      "                \"paragraph\": 2,\n",
      "                \"text\": \"users can 12/23/24, 2:22 PM LLMs Meet Misinformation https://llm-misinformation.github.io 8/15\"\n",
      "            }\n",
      "        ]\n",
      "    },\n",
      "    {\n",
      "        \"page_number\": 9,\n",
      "        \"chunks\": [\n",
      "            {\n",
      "                \"paragraph\": 1,\n",
      "                \"text\": \"easily utilize methods shown in Table 1 and close-sourced LLMs (e.g., ChatGPT) or open- sourced LLMs (e.g., Llama2 (Touvron et al., 2023b)) to generate misinformation at scale in different domains, types, and errors, which is hard for conventional supervisedly trained detectors to maintain effective. Thus, it is likely to be impractical to apply conventional supervisedly trained detectors to detect LLM-generated misinformation in the practices. Evaluation Setting: We adopt LLMs such as GPT-4 with zero-shot prompting strategies as the representative misinformation detectors to assess and compare the detection hardness of LLMgenerated misinformation and human-written misinformation for two reasons. First, zero-shot setting can better reflect the real-world scenarios of detecting LLM-generated misinformation considering the likely impracticality of conventional supervisedly trained detectors (e.g., BERT) in practices. Second, there are many works that have demonstrated directly prompting LLMs such as GPT-4 in a zero-shot way can outperform conventional supervisedly trained models such as BERT on detecting human- written misinformation (Pelrine et al., 2023; Zhang et al., 2023c; Bang et al., 2023; Buchholz, 2023; Li et al., 2023b), which shows that zero-shot LLMs have already achieved almost state-of-the-art performance in the task of misinformation detection. In the zero- shot setting, we can adopt Success Rate\"\n",
      "            },\n",
      "            {\n",
      "                \"paragraph\": 2,\n",
      "                \"text\": \"% as the metric to measure the probability of LLM-generated or human-written misinformation being successfully identified, representing the difficulty of being detected. Experiment Result Analysis: First, we can observe that it is also generally hard for LLM detectors to detect ChatGPT-generated misinformation, especially those generated via Hallucinated News Generation, Totally Arbitrary Generation and Open-ended Generation. For example, LLM detectors can hardly detect fine-grained hallucinations. Second, previous works have shown that detectors can perform better than humans on detecting human-written misinformation (Pe\\u0301rez-Rosas et al., 2018). Comparing LLM detection and human detection performance, we can discover that GPT-4 can outperform humans on detecting ChatGPT-generated misinformation, though humans can still perform better than ChatGPT-3.5. 12/23/24, 2:22 PM LLMs Meet Misinformation https://llm-misinformation.github.io 9/15\"\n",
      "            }\n",
      "        ]\n",
      "    },\n",
      "    {\n",
      "        \"page_number\": 10,\n",
      "        \"chunks\": [\n",
      "            {\n",
      "                \"paragraph\": 1,\n",
      "                \"text\": \"After evaluating the overall performance of LLM detectors, we aim to further investigate whether or not LLM-generated misinformation can be harder for detectors to detect than human-written misinformation with the same semantics. As shown in Table 4, we can observe that the LLM detection performances on ChatGPT- generated misinformation are mostly lower than those on human-written misinformation. For example, Llama2-7B with \\\"CoT\\\" has a performance drop by 19.6% on detecting misinformation generated via Rewriting Generation based on Politifact compared with detecting human-written misinformation. Thus, we can have our third core finding: LLM-generated misinformation can be harder for misinformation detectors to detect than human-written misinformation with the same semantics. Our finding implies that LLM-generated misinformation can have more deceptive styles for detectors and existing detectors are likely to be less effective in detecting LLM- generated misinformation. Also, malicious users could potentially utilize LLMs to escape the detection of detectors. 12/23/24, 2:22 PM LLMs Meet Misinformation https://llm-misinformation.github.io 10/15\"\n",
      "            }\n",
      "        ]\n",
      "    },\n",
      "    {\n",
      "        \"page_number\": 11,\n",
      "        \"chunks\": [\n",
      "            {\n",
      "                \"paragraph\": 1,\n",
      "                \"text\": \"Implications on Combating Misinformation at the Age of LLMs Implication 1: our findings directly suggest that humans can be more susceptible to LLM- generated misinformation and detectors can be less effective in detecting LLM- generated misinformation compared with human-written misinformation. In other words, LLM-generated misinformation can be more deceptive and potentially cause more harm. Implication 2: on the one hand, a large amount of hallucinated information is potentially generated by normal users due to the popularity of LLMs. On the other hand, malicious users are more likely to exploit LLMs to generate misinformation to escape the detection of detectors. Thus, there is a potential major paradigm shift of misinformation production from humans to LLMs. Implication 3: considering malicious users can easily prompt LLMs to generate misinformation at scale, which is more deceptive than human-written misinformation, online safety and public trust are faced with serious threats. We call for collective efforts 12/23/24, 2:22 PM LLMs Meet Misinformation https://llm-misinformation.github.io 11/15\"\n",
      "            }\n",
      "        ]\n",
      "    },\n",
      "    {\n",
      "        \"page_number\": 12,\n",
      "        \"chunks\": [\n",
      "            {\n",
      "                \"paragraph\": 1,\n",
      "                \"text\": \"on combating LLM-generated misinformation from stakeholders in different backgrounds including researchers, government, platforms, and the general public. Countermeasures Through LLMs\\u2019 Lifecycle As shown in Figure 7, we propose to divide the lifecycle of LLMs into three stages and discuss the countermeasures against LLM-generated misinformation through the whole lifecycle. In the training stage, we can curate the training data to remove nonfactual articles and ground the training process to existing knowledge bases (Yu et al., 2020) to reduce LLMs\\u2019 hallucinations. Alignment training processes such as RLHF (Casper et al., 2023a) can reduce the risk of generating harmful content. In the Inference stage, we can utilize prompt filtering, intent modeling or jailbreak defenses (Jain et al., 2023) to prevent AMG methods (e.g., Totally Arbitrary Generation), and confidence (or uncertainty) estimation (Xiong et al., 2023) or retrieval augmentation (Mialon et al., 2023) to defend against HG methods (e.g., Hallucinated News Generation). However, they may be ineffective for most of CMG methods (e.g., Rewriting Generation), which are based on human-written misleading content and do not explicitly express the intent of generating misinformation. More research is desired to develop inference-time factually verification methods for combating CMG methods. In the influence stage when LLM-generated content starts\"\n",
      "            },\n",
      "            {\n",
      "                \"paragraph\": 2,\n",
      "                \"text\": \"to influence the public, it is under-explored how to design effective detectors for LLM-generated misinformation or texts. Also, it is essential to enhance the public\\u2019s awareness of LLM-generated misinformation. Ethics Statement Considering that the open-sourced LLMs (e.g., Llama) or close-sourced LLMs (e.g., ChatGPT) are widely adopted, and the potential approaches to generate misinformation with LLMs are based on real-world scenarios (shown in Table 1) and straightforward to implement, we anticipate these methods have been potentially utilized to generate 12/23/24, 2:22 PM LLMs Meet Misinformation https://llm-misinformation.github.io 12/15\"\n",
      "            }\n",
      "        ]\n",
      "    },\n",
      "    {\n",
      "        \"page_number\": 13,\n",
      "        \"chunks\": [\n",
      "            {\n",
      "                \"paragraph\": 1,\n",
      "                \"text\": \"misinformation by normal people unintentionally or malicious users intentionally in the real world. Thus, our research illustrates the landscape of LLM-generated misinformation to shed light on the potential risks, enhance the public\\u2019s awareness of its harm, and call for collective countering efforts. We also discuss the implications of our findings and the potential countermeasures, which can inspire and facilitate more future research on defending against LLM-generated misinformation. Examples of ChatGPT-Generated Misinformation PROMPT: You are a journalist. Please write a piece of news. RESPONSE: Headline: New Study Reveals Surprising Benefits of Gardening for Mental Health. Date: October 12, 2022. In a fresh breakthrough highlighting the importance of wellbeing, a landmark study conducted by experts at the renowned Garden Research Institute (GRI) has unearthed remarkable benefits of gardening on mental health. The study, conducted over a span of three years, reveals how nurturing nature can play a pivotal role in combating stress, anxiety, and depression. Gardening enthusiasts and mental health professionals have long acknowledged the therapeutic effects of spending time amid plants and flowers, but this study delves deeper to emphasize the extent of the positive mental health impact. The GRI study included over 2,000 participants from diverse age groups, socio- economic\"\n",
      "            },\n",
      "            {\n",
      "                \"paragraph\": 2,\n",
      "                \"text\": \"backgrounds, and geographical locations. Researchers used a combination of quantitative surveys and qualitative interviews to gather a comprehensive understanding of the psychological benefits that gardening brings. The findings unveiled an array of advantages that can be attested to gardening. First and foremost, participants reported a significant decrease in stress levels. The act of gardening was shown to encourage relaxation, helping individuals unwind and find solace amidst the natural world. Moreover, the study demonstrated that gardening had the power to stabilize moods, with over 81% of participants experiencing reduced feelings of anxiety and depression. Interestingly, the therapeutic connection between people and plants wasn\\u2019t limited to the emotional realm. The GRI study indicated a tangible positive correlation between gardening Example 1 of Hallucinated News Generation method. 12/23/24, 2:22 PM LLMs Meet Misinformation https://llm-misinformation.github.io 13/15\"\n",
      "            }\n",
      "        ]\n",
      "    },\n",
      "    {\n",
      "        \"page_number\": 14,\n",
      "        \"chunks\": [\n",
      "            {\n",
      "                \"paragraph\": 1,\n",
      "                \"text\": \"and physical well-being. Physical ailments such as high blood pressure, sleep disorders, and chronic pain were reported with lower frequency among participants who engaged in regular gardening activities. Elaborating on the findings, Dr. Emily Stevens, lead researcher at GRI, emphasized, \\u201cGardening provides a multi- faceted approach to improving mental health. It combines the benefits of physical exercise, connecting with nature, and nurturing living organisms, which ultimately empowers individuals to regain a sense of control and purpose in their lives, leading to improved mental well-being.\\u201d The study also highlighted the social aspects associated with gardening, especially within community gardens or allotments. Participants expressed a higher sense of community belonging, reduced feelings of isolation, and increased social interactions. Gardening, it seems, has the potential to bring people together, fostering a stronger sense of unity and shared purpose. Acknowledgement This material is based upon work supported by the U.S. Department of Homeland Security under Grant Award Number 17STQAC00001-07-04, and the Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), via the HIATUS Program contract #2022-22072200001, NSF SaTC-2241068, a Cisco Research Award, a Microsoft Accelerate Foundation Models Research Award. The views and conclusions contained in this document are those\"\n",
      "            },\n",
      "            {\n",
      "                \"paragraph\": 2,\n",
      "                \"text\": \"of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of the U.S. Department of Homeland Security, ODNI, IARPA, or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for governmental purposes notwithstanding any copyright annotation therein. BibTeX @inproceedings{chen2024llmgenerated, title={Can {LLM}-Generated Misinformation Be Detected?}, author={Canyu Chen and Kai Shu}, booktitle={The Twelfth International Conference on Learning Representations year={2024}, 12/23/24, 2:22 PM LLMs Meet Misinformation https://llm-misinformation.github.io 14/15\"\n",
      "            }\n",
      "        ]\n",
      "    },\n",
      "    {\n",
      "        \"page_number\": 15,\n",
      "        \"chunks\": [\n",
      "            {\n",
      "                \"paragraph\": 1,\n",
      "                \"text\": \"url={https://openreview.net/forum?id=ccxD4mtkTU} } This website is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License. This means you are free to borrow the source code of this website, we just ask that you link back to this page in the footer. Please remember to remove the analytics code included in the header of the website which you do not want on your website. 12/23/24, 2:22 PM LLMs Meet Misinformation https://llm-misinformation.github.io 15/15\"\n",
      "            }\n",
      "        ]\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# Load the JSON data from a file\n",
    "with open('extracted_data.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Display the loaded JSON data (optional for verification)\n",
    "print(json.dumps(data, indent=4))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Define a Function to Filter Relevant Context\n",
    "\n",
    "Now, we define a function to filter the relevant context based on a given question. This function will use keyword matching to find relevant sentences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cosine_similarity(question: str, text: str) -> float:\n",
    "    \"\"\"\n",
    "    Calculate cosine similarity between the question and the text.\n",
    "    \n",
    "    Args:\n",
    "    - question (str): The question to compare.\n",
    "    - text (str): The text to compare.\n",
    "    \n",
    "    Returns:\n",
    "    - float: The cosine similarity between the question and the text.\n",
    "    \"\"\"\n",
    "    # Initialize TF-IDF Vectorizer\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "    # Vectorize the question and the text\n",
    "    tfidf_matrix = vectorizer.fit_transform([question, text])\n",
    "\n",
    "    # Calculate cosine similarity\n",
    "    cosine_sim = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])\n",
    "    return cosine_sim[0][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"page_number\": 3,\n",
      "        \"chunks\": [\n",
      "            {\n",
      "                \"paragraph\": 1,\n",
      "                \"text\": \"Abstract Misinformation such as fake news and rumors is a serious threat to information ecosystems and public trust. The emergence of Large Language Models (LLMs) has great potential to reshape the landscape of combating misinformation. Generally, LLMs can be a double-edged sword in the fight. On the one hand, LLMs bring promising opportunities for combating misinformation due to their profound world knowledge and strong reasoning abilities. Thus, one emergent question is: can we utilize LLMs to combat misinformation? On the other hand, the critical challenge is that LLMs can be easily leveraged to generate deceptive misinformation at scale. Then, another important question is: how to combat LLM-generated misinformation? In this paper, we first systematically review the history of combating misinformation before the advent of LLMs. Then we illustrate the current efforts and present an outlook for these two fundamental questions respectively. The goal of this survey paper is to facilitate the progress of utilizing LLMs for fighting misinformation and call for interdisciplinary efforts from different stakeholders for combating LLM-generated misinformation. BibTeX @article{chen2024combatingmisinformation, author = {Chen, Canyu and Shu, Kai}, title = {Combating misinformation in the age of LLMs: Opportunities and cha journal = {AI Magazine}, year = {2024}, doi =\",\n",
      "                \"similarity\": 0.5128128580371095\n",
      "            }\n",
      "        ]\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "def filter_content_by_similarity(data: List[Dict], question: str, threshold: float = 0.5) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Filter the JSON content based on cosine similarity with the question.\n",
    "    \n",
    "    Args:\n",
    "    - data (List[Dict]): The JSON data.\n",
    "    - question (str): The question to filter content against.\n",
    "    - threshold (float): The similarity threshold for filtering.\n",
    "    \n",
    "    Returns:\n",
    "    - List[Dict]: Filtered content with relevant paragraphs having cosine similarity above the threshold.\n",
    "    \"\"\"\n",
    "    filtered_data = []\n",
    "\n",
    "    for page in data:\n",
    "        page_number = page['page_number']\n",
    "        relevant_chunks = []\n",
    "        for chunk in page['chunks']:\n",
    "            similarity = calculate_cosine_similarity(question, chunk['text'])\n",
    "            if similarity >= threshold:\n",
    "                relevant_chunks.append({\n",
    "                    'paragraph': chunk['paragraph'],\n",
    "                    'text': chunk['text'],\n",
    "                    'similarity': similarity\n",
    "                })\n",
    "        \n",
    "        if relevant_chunks:\n",
    "            filtered_page = {\n",
    "                'page_number': page_number,\n",
    "                'chunks': relevant_chunks\n",
    "            }\n",
    "            filtered_data.append(filtered_page)\n",
    "    \n",
    "    return filtered_data\n",
    "\n",
    "# Example usage: Filter content based on cosine similarity with the question\n",
    "question = \"How can LLMs help in combating misinformation?\"\n",
    "filtered_data = filter_content_by_similarity(data, question)\n",
    "\n",
    "# Display the filtered data (optional for verification)\n",
    "print(json.dumps(filtered_data, indent=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Preparing Context for LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Page 3 ---\n",
      "Paragraph 1:\n",
      "Abstract Misinformation such as fake news and rumors is a serious threat to information ecosystems and public trust. The emergence of Large Language Models (LLMs) has great potential to reshape the landscape of combating misinformation. Generally, LLMs can be a double-edged sword in the fight. On the one hand, LLMs bring promising opportunities for combating misinformation due to their profound world knowledge and strong reasoning abilities. Thus, one emergent question is: can we utilize LLMs to combat misinformation? On the other hand, the critical challenge is that LLMs can be easily leveraged to generate deceptive misinformation at scale. Then, another important question is: how to combat LLM-generated misinformation? In this paper, we first systematically review the history of combating misinformation before the advent of LLMs. Then we illustrate the current efforts and present an outlook for these two fundamental questions respectively. The goal of this survey paper is to facilitate the progress of utilizing LLMs for fighting misinformation and call for interdisciplinary efforts from different stakeholders for combating LLM-generated misinformation. BibTeX @article{chen2024combatingmisinformation, author = {Chen, Canyu and Shu, Kai}, title = {Combating misinformation in the age of LLMs: Opportunities and cha journal = {AI Magazine}, year = {2024}, doi =\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def prepare_context(filtered_data: List[Dict]) -> str:\n",
    "    \"\"\"\n",
    "    Prepare context for LLM question answering by concatenating relevant text chunks.\n",
    "    \n",
    "    Args:\n",
    "    - filtered_data (List[Dict]): The filtered content data.\n",
    "    \n",
    "    Returns:\n",
    "    - str: The concatenated context for LLM.\n",
    "    \"\"\"\n",
    "    context = \"\"\n",
    "    \n",
    "    for page in filtered_data:\n",
    "        page_number = page['page_number']\n",
    "        context += f'--- Page {page_number} ---\\n'\n",
    "        \n",
    "        for chunk in page['chunks']:\n",
    "            paragraph_number = chunk['paragraph']\n",
    "            text = chunk['text']\n",
    "            context += f'Paragraph {paragraph_number}:\\n{text}\\n\\n'\n",
    "    \n",
    "    return context\n",
    "\n",
    "# Prepare the context from the filtered data\n",
    "context = prepare_context(filtered_data)\n",
    "\n",
    "# Display the context (optional for verification)\n",
    "print(context)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Question Answering with LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use llm\n",
    "\n",
    "from openai import OpenAI\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How can LLMs help in combating misinformation?\n",
      "Answer: <think>\n",
      "Okay, so I need to figure out how LLMs can help combat misinformation based on the given context. Let's start by reading through the context carefully. The context is from page 3 of a paper by Chen and Shu from 2024. The first paragraph talks about how LLMs are a double-edged sword—they can help fight misinformation but also generate it. The main questions are whether we can use LLMs to combat misinformation and how to deal with the misinformation they might create.\n",
      "\n",
      "The user's question is specifically asking about how LLMs can help, so I should focus on the opportunities mentioned. The context says LLMs have \"profound world knowledge and strong reasoning abilities,\" which are good for combating misinformation. The paper also mentions that they review past efforts before LLMs and discuss current efforts and outlooks for the two questions. The goal is to promote using LLMs against misinformation and call for interdisciplinary efforts against LLM-generated misinformation.\n",
      "\n",
      "Now, I need to parse out the key points. The opportunities are their knowledge and reasoning. Probably, this means LLMs can fact-check, verify information sources, detect inconsistencies, maybe even generate counter-narratives. Also, since they can process large amounts of data, they might identify patterns of misinformation. But I should stick to what's in the context. The context doesn't go into specifics but mentions promising opportunities. The answer should highlight their knowledge and reasoning as the main factors. Also, the paper's goal is to use LLMs to fight misinformation, so maybe they can be tools for detection and correction.\n",
      "\n",
      "Wait, the context also mentions that the challenge is LLMs generating misinformation. But the question is about how they can help, so focus on the positive aspects. The answer should include the two points: world knowledge and reasoning abilities. Maybe also mention that they can be used to detect fake news by analyzing text for inconsistencies or cross-referencing with reliable sources. The paper's abstract is structured into the two questions, so the answer should probably mention the opportunities first. The user wants the answer based on the context, so I can't add external information. The answer needs to be concise, using the provided context.\n",
      "\n",
      "Let me check again. The context says: \"On the one hand, LLMs bring promising opportunities for combating misinformation due to their profound world knowledge and strong reasoning abilities. Thus, one emergent question is: can we utilize LLMs to combat misinformation?\" The answer should explain that because of their knowledge and reasoning, LLMs can be used to detect and combat misinformation, perhaps through verification, fact-checking, identifying patterns, etc. The context doesn't provide specific methods but mentions these general capabilities.\n",
      "\n",
      "Therefore, the answer would outline that LLMs can combat misinformation by leveraging their extensive knowledge bases to verify facts and use reasoning to analyze information credibility. They might automate the detection of false claims, cross-check information against trusted sources, and generate accurate content to counteract misinformation. The paper's survey likely reviews these aspects, so summarizing those points based on the abstract given.\n",
      "</think>\n",
      "\n",
      "Based on the provided context, **Large Language Models (LLMs)** can help combat misinformation in the following ways:  \n",
      "\n",
      "1. **Leveraging Knowledge and Reasoning**:  \n",
      "   LLMs possess **profound world knowledge** and **strong reasoning abilities**, enabling them to analyze and verify information effectively. They can cross-reference claims against verified data, detect logical inconsistencies, and identify patterns indicative of misinformation (e.g., fake news, rumors).  \n",
      "\n",
      "2. **Automated Detection and Fact-Checking**:  \n",
      "   By processing vast amounts of text, LLMs can assist in **automated detection of false claims** and act as scalable tools for fact-checking. Their ability to synthesize and contextualize information allows them to prioritize suspicious content for human review.  \n",
      "\n",
      "3. **Proactive Mitigation**:  \n",
      "   The paper suggests that LLMs could generate **accurate, trustworthy content** to counteract misinformation, thereby promoting reliable information ecosystems.  \n",
      "\n",
      "However, the context also highlights the dual nature of LLMs: while they offer these opportunities, they can also **exacerbate misinformation risks** if misused to generate deceptive content at scale. The authors emphasize the need for **interdisciplinary efforts** (technical, ethical, and regulatory) to harness LLMs responsibly and address challenges like LLM-generated misinformation.  \n",
      "\n",
      "*Source*: Chen & Shu (2024), *AI Magazine* survey on combating misinformation with LLMs.\n"
     ]
    }
   ],
   "source": [
    "def get_answer_from_llm(question: str, context: str) -> str:\n",
    "    \"\"\"\n",
    "    Get an answer from the LLM given a question and context.\n",
    "    \n",
    "    Args:\n",
    "    - question (str): The question to ask the LLM.\n",
    "    - context (str): The context to provide to the LLM.\n",
    "    \n",
    "    Returns:\n",
    "    - str: The answer from the LLM.\n",
    "    \"\"\"\n",
    "    client = OpenAI(\n",
    "\tbase_url=\"https://huggingface.co/api/inference-proxy/together\",\n",
    "\tapi_key=\"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\"\n",
    "    )\n",
    "\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"you are a Question answer bot. Answer the following question:{question} based on the context below\\n{context}\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"deepseek-ai/DeepSeek-R1\",\n",
    "        messages=messages,\n",
    "        max_tokens=5000\n",
    "    )\n",
    "    \n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "# Example question\n",
    "question = \"How can LLMs help in combating misinformation?\"\n",
    "\n",
    "# Get the answer from the LLM\n",
    "answer = get_answer_from_llm(question, context)\n",
    "\n",
    "# Display the answer\n",
    "print(f'Question: {question}')\n",
    "print(f'Answer: {answer}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ihackenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
