{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDF Parsing and Cleaning\n",
    "\n",
    "This notebook demonstrates how to parse and clean text from a PDF document using Python libraries such as `PyMuPDF` and `nltk`.\n",
    "\n",
    "## Step 1: Install Required Libraries\n",
    "\n",
    "First, we need to install the necessary libraries. Run the following command in your terminal or in a Jupyter Notebook cell:\n",
    "\n",
    "```python\n",
    "!pip install PyMuPDF nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PyMuPDF\n",
      "  Obtaining dependency information for PyMuPDF from https://files.pythonhosted.org/packages/46/72/8c5bbf817aacebe21a454f3ade8ee4b5b17afe698bb73d65c4ca23a89a87/pymupdf-1.25.1-cp39-abi3-win_amd64.whl.metadata\n",
      "  Downloading pymupdf-1.25.1-cp39-abi3-win_amd64.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: nltk in c:\\users\\paridasubidk\\appdata\\local\\anaconda\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\paridasubidk\\appdata\\local\\anaconda\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\paridasubidk\\appdata\\local\\anaconda\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\paridasubidk\\appdata\\local\\anaconda\\lib\\site-packages (from nltk) (2024.7.24)\n",
      "Requirement already satisfied: tqdm in c:\\users\\paridasubidk\\appdata\\local\\anaconda\\lib\\site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\paridasubidk\\appdata\\local\\anaconda\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Downloading pymupdf-1.25.1-cp39-abi3-win_amd64.whl (16.6 MB)\n",
      "   ---------------------------------------- 0.0/16.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/16.6 MB 1.3 MB/s eta 0:00:13\n",
      "   ---------------------------------------- 0.1/16.6 MB 1.3 MB/s eta 0:00:13\n",
      "    --------------------------------------- 0.2/16.6 MB 2.0 MB/s eta 0:00:09\n",
      "   - -------------------------------------- 0.5/16.6 MB 3.2 MB/s eta 0:00:06\n",
      "   -- ------------------------------------- 1.0/16.6 MB 5.0 MB/s eta 0:00:04\n",
      "   --- ------------------------------------ 1.4/16.6 MB 5.7 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 2.4/16.6 MB 8.0 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 3.3/16.6 MB 9.6 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 3.6/16.6 MB 9.2 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 4.0/16.6 MB 9.9 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 4.1/16.6 MB 8.4 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 5.3/16.6 MB 10.0 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 6.8/16.6 MB 11.7 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 7.6/16.6 MB 12.2 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 8.7/16.6 MB 13.0 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 9.5/16.6 MB 13.2 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 10.3/16.6 MB 13.6 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 10.9/16.6 MB 16.4 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 11.1/16.6 MB 16.0 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 11.1/16.6 MB 16.0 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 11.8/16.6 MB 14.9 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 13.8/16.6 MB 17.3 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 14.5/16.6 MB 18.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 15.5/16.6 MB 18.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 16.1/16.6 MB 17.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  16.6/16.6 MB 16.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 16.6/16.6 MB 14.9 MB/s eta 0:00:00\n",
      "Installing collected packages: PyMuPDF\n",
      "Successfully installed PyMuPDF-1.25.1\n"
     ]
    }
   ],
   "source": [
    "# !pip install PyMuPDF nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Next, we will import the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import re\n",
    "import json\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Parsing PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The PDF document has 15 pages.\n"
     ]
    }
   ],
   "source": [
    "# Specify the path to your PDF file\n",
    "pdf_path = 'llms.pdf'\n",
    "\n",
    "# Open the PDF document\n",
    "pdf_document = fitz.open(pdf_path)\n",
    "\n",
    "# Check the number of pages\n",
    "num_pages = pdf_document.page_count\n",
    "print(f'The PDF document has {num_pages} pages.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Text Page by Page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Page 1 ---\n",
      "This is an initiative aiming to combat misinformation in the age\n",
      "of LLMs\n",
      "(Correspondence to: Kai Shu)\n",
      "(New Preprint) Can Knowledge Editing Really Correct Hallucinations?\n",
      "- We proposed HalluEditBench to holistically benchmark knowledge\n",
      "editing methods in correcting real-world hallucinations on five\n",
      "dimensions including Efficacy, Generalization, Portability, Locality, and\n",
      "Robustness. We find their effectiveness could be far from what their\n",
      "performance on existing datasets suggests, and the performance\n",
      "beyond Efficacy for all methods is generally unsatisfactory.\n",
      "(New Preprint) Can Editing LLMs Inject Harm?\n",
      "- We propose to reformulate knowledge editing as a new type of safety\n",
      "threat for LLMs, namely Editing Attack, and discover its emerging risk\n",
      "of injecting misinformation or bias into LLMs stealthily, indicating the\n",
      "feasibility of disseminating misinformation or bias with LLMs as new\n",
      "channels.\n",
      "(SIGKDD Explorations 2024) Authorship Attribution in the Era of LLMs:\n",
      "Problems, Methodologies, and Challenges\n",
      "- This survey paper systematically categorizes authorship attribution in\n",
      "the era of LLMs into four problems: attributing unknown texts to\n",
      "human authors, detecting LLM-generated texts, identifying specific\n",
      "LLMs or human authors, and classifying texts as human-authored,\n",
      "machine-generated, or co-authored by both, while also highlighting\n",
      "key challenges and open problems.\n",
      "(EMNLP 2024 Findings) Can Large Language Models Identify\n",
      "Authorship?\n",
      "- We propose Linguistically Informed Prompting (LIP) strategy, which\n",
      "offers in-context linguistic guidance, to boost LLMs' reasoning capacity\n",
      "for authorship verification and attribution tasks, while also providing\n",
      "natural language explanations.\n",
      "(AI Magazine 2024) Combating Misinformation in the Age of LLMs:\n",
      "Opportunities and Challenges\n",
      "- A survey of the opportunities (can we utilize LLMs to combat\n",
      "misinformation) and challenges (how to combat LLM-generated\n",
      "misinformation) of combating misinformation in the age of LLMs.\n",
      "(Proceedings of ICLR 2024) Can LLM-Generated Misinformation Be\n",
      "12/23/24, 2:22 PM\n",
      "LLMs Meet Misinformation\n",
      "https://llm-misinformation.github.io\n",
      "1/15\n",
      "\n",
      "\n",
      "\n",
      "--- Page 2 ---\n",
      "Detected?\n",
      "- We discover that LLM-generated misinformation can be harder to\n",
      "detect for humans and detectors compared to human-written\n",
      "misinformation with the same semantics, which suggests it can have\n",
      "more deceptive styles and potentially cause more harm.\n",
      "Combating Misinformation in the Age\n",
      "of LLMs: Opportunities and\n",
      "Challenges\n",
      "Canyu Chen, Kai Shu \n",
      "Illinois Institute of Technology\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Published at AI Magazine 2024 (Volume 45, Issue 3, Fall 2024), Highlight\n",
      "Article\n",
      "Publication\n",
      "Paper\n",
      "arXiv\n",
      "Talk\n",
      "Slides 1\n",
      "Slides 2\n",
      "Paper List\n",
      "12/23/24, 2:22 PM\n",
      "LLMs Meet Misinformation\n",
      "https://llm-misinformation.github.io\n",
      "2/15\n",
      "\n",
      "\n",
      "\n",
      "--- Page 3 ---\n",
      "Abstract\n",
      "Misinformation such as fake news and rumors is a serious threat to\n",
      "information ecosystems and public trust. The emergence of Large\n",
      "Language Models (LLMs) has great potential to reshape the landscape\n",
      "of combating misinformation. Generally, LLMs can be a double-edged\n",
      "sword in the fight. On the one hand, LLMs bring promising\n",
      "opportunities for combating misinformation due to their profound\n",
      "world knowledge and strong reasoning abilities. Thus, one emergent\n",
      "question is: can we utilize LLMs to combat misinformation? On the\n",
      "other hand, the critical challenge is that LLMs can be easily leveraged\n",
      "to generate deceptive misinformation at scale. Then, another\n",
      "important question is: how to combat LLM-generated misinformation?\n",
      "In this paper, we first systematically review the history of combating\n",
      "misinformation before the advent of LLMs. Then we illustrate the\n",
      "current efforts and present an outlook for these two fundamental\n",
      "questions respectively. The goal of this survey paper is to facilitate the\n",
      "progress of utilizing LLMs for fighting misinformation and call for\n",
      "interdisciplinary efforts from different stakeholders for combating\n",
      "LLM-generated misinformation.\n",
      "BibTeX\n",
      "@article{chen2024combatingmisinformation,\n",
      "      author = {Chen, Canyu and Shu, Kai},\n",
      "      title = {Combating misinformation in the age of LLMs: Opportunities and cha\n",
      "      journal = {AI Magazine},\n",
      "      year = {2024},\n",
      "      doi = {10.1002/aaai.12188},\n",
      "      url = {https://doi.org/10.1002/aaai.12188}\n",
      "      }\n",
      "12/23/24, 2:22 PM\n",
      "LLMs Meet Misinformation\n",
      "https://llm-misinformation.github.io\n",
      "3/15\n",
      "\n",
      "\n",
      "\n",
      "--- Page 4 ---\n",
      "Can LLM-Generated Misinformation\n",
      "Be Detected?\n",
      "Canyu Chen, Kai Shu \n",
      "Illinois Institute of Technology\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Published at Proceedings of ICLR 2024\n",
      "Publication\n",
      "Paper\n",
      "arXiv\n",
      "Dataset and Code\n",
      "Talk\n",
      "Slides 1\n",
      "Slides 2\n",
      "post\n",
      "post\n",
      "post\n",
      "Abstract\n",
      "The advent of Large Language Models (LLMs) has made a\n",
      "transformative impact. However, the potential that LLMs such as\n",
      "ChatGPT can be exploited to generate misinformation has posed a\n",
      "serious concern to online safety and public trust. A fundamental\n",
      "research question is: will LLM-generated misinformation cause more\n",
      "harm than human-written misinformation? We propose to tackle this\n",
      "question from the perspective of detection difficulty. We first build a\n",
      "taxonomy of LLM-generated misinformation. Then we categorize and\n",
      "validate \n",
      "the \n",
      "potential \n",
      "real-world \n",
      "methods \n",
      "for \n",
      "generating\n",
      "12/23/24, 2:22 PM\n",
      "LLMs Meet Misinformation\n",
      "https://llm-misinformation.github.io\n",
      "4/15\n",
      "\n",
      "\n",
      "\n",
      "--- Page 5 ---\n",
      "misinformation with LLMs. Then, through extensive empirical\n",
      "investigation, we discover that LLM-generated misinformation can be\n",
      "harder to detect for humans and detectors compared to human-\n",
      "written misinformation with the same semantics, which suggests it\n",
      "can have more deceptive styles and potentially cause more harm. We\n",
      "also discuss the implications of our discovery on combating\n",
      "misinformation in the age of LLMs and the countermeasures.\n",
      "Our Contributions\n",
      "(1) We build a taxonomy by types, domains, sources, intents and errors to systematically\n",
      "characterize LLM-generated misinformation as an emerging and critical research topic.\n",
      "(2) We make the first attempt to categorize and validate the potential real-world methods\n",
      "for generating misinformation with LLMs including Hallucination Generation, Arbitrary\n",
      "Misinformation Generation and Controllable Misinformation Generation methods.\n",
      "(3) We discover that misinformation generated by LLMs can be harder for humans and\n",
      "detectors to detect than human-written misinformation with the same semantic\n",
      "information through extensive investigation, which provides sufficient empirical evidence\n",
      "to demonstrate that LLM-generated misinformation can have more deceptive styles and\n",
      "potentially cause more harm.\n",
      "(4) We discuss the emerging challenges for misinformation detectors (Section 6),\n",
      "important implications of our discovery on combating misinformation in the age of LLMs\n",
      "(Section 7), the countermeasures against LLM-generated misinformation through LLMs’\n",
      "whole lifecycle (Section 8).\n",
      "Taxonomy of LLM-Generated\n",
      "Misinformation\n",
      "We propose to taxonomize LLM-generated misinformation from five dimensions\n",
      "including types, domains, sources, intents and errors. In particular, we categorize the\n",
      "sources of LLM-generated misinformation into hallucination, arbitrary generation and\n",
      "controllable generation since there are different potential methods to generate\n",
      "misinformation with LLMs. Also, we divide the intents of generated misinformation into\n",
      "unintentional and intentional generation considering hallucination can potentially occur\n",
      "in any generation process of LLMs and users without malicious intent may also generate\n",
      "texts containing hallucinated information when using LLMs.\n",
      "12/23/24, 2:22 PM\n",
      "LLMs Meet Misinformation\n",
      "https://llm-misinformation.github.io\n",
      "5/15\n",
      "\n",
      "\n",
      "\n",
      "--- Page 6 ---\n",
      "RQ1: How Can LLMs be Utilized to Generate\n",
      "Misinformation?\n",
      "We propose to categorize the LLM-based misinformation generation methods into three\n",
      "types based on real-world scenarios (Table 1): Hallucination Generation (HG), Arbitrary\n",
      "Misinformation Generation (AMG) and Controllable Misinformation Generation (CMG).\n",
      "12/23/24, 2:22 PM\n",
      "LLMs Meet Misinformation\n",
      "https://llm-misinformation.github.io\n",
      "6/15\n",
      "\n",
      "\n",
      "\n",
      "--- Page 7 ---\n",
      "Connection with Jailbreak Attack: Jailbreak attacks usually refer to the attempts to bypass\n",
      "the safety guards of LLMs (e.g., ChatGPT) to generate harmful content. On the one hand,\n",
      "our proposed approaches to generate misinformation with LLMs are motivated by real-\n",
      "world scenarios shown in Table 1 and orthogonal to the previous Jailbreak techniques\n",
      "(Wei et al., 2023a; Zou et al., 2023), which suggests the misinformation generation\n",
      "approaches and previous jailbreak methods could be potentially combined by attackers.\n",
      "On the other hand, the HG methods could be regarded as Unintentional Jailbreak, which\n",
      "is different from most previous jailbreak methods. The AMG and CMG methods could be\n",
      "regarded as Intentional Jailbreak.\n",
      "We test the possibilities that our misinformation generation approaches can bypass the\n",
      "safeguard of ChatGPT by prompting with each approach for 100 times. The Attacking\n",
      "Success Rates are in Table 2. Thus, our first core finding is:\n",
      "LLMs can follow users’ instructions to generate misinformation in different\n",
      "types, domains, and errors.\n",
      "RQ2: Can Humans Detect LLM-Generated\n",
      "Misinformation?\n",
      "Although previous works have shown that it is hard for humans to detect human-written\n",
      "misinformation (Lyons et al., 2021), it is still under-explored whether or not humans can\n",
      "detect LLM-generated misinformation.\n",
      "Experiment Result Analysis: First, we can observe that it is generally hard for humans to\n",
      "detect ChatGPT-generated misinformation, especially those generated with Hallucinated\n",
      "News Generation, Totally Arbitrary Generation, Rewriting Generation, and Open-ended\n",
      "Generation.\n",
      "12/23/24, 2:22 PM\n",
      "LLMs Meet Misinformation\n",
      "https://llm-misinformation.github.io\n",
      "7/15\n",
      "\n",
      "\n",
      "\n",
      "--- Page 8 ---\n",
      "Second, we attempt to compare the human detection’s hardness for ChatGPT-generated\n",
      "and human-written misinformation that have the same semantics. We have\n",
      "demonstrated that Paraphrase Generation, Rewriting Generation, and Open-ended\n",
      "Generation generally only change the style information and preserve the original\n",
      "semantics. Comparing human detection performance on human-written misinformation\n",
      "(the grey numbers in Table 3) and ChatGPT-generated misinformation via Paraphrase\n",
      "Generation, Rewriting Generation and Open-ended Generation approaches (the red or\n",
      "green numbers in Table 3), we can discover that the human detection performances on\n",
      "ChatGPT-generated misinformation are mostly lower than those on human-written\n",
      "misinformation. Thus, we can have our second core finding shown as follows:\n",
      "LLM-generated misinformation can be harder for humans to detect than\n",
      "human-written misinformation with the same semantics.\n",
      "Our finding also validates that LLM-generated misinformation can have more deceptive\n",
      "styles for humans and implies humans can be potentially more susceptible to LLM-\n",
      "generated misinformation than human-written misinformation.\n",
      "RQ3: Can Detectors Detect LLM-Generated\n",
      "Misinformation?\n",
      "Emerging Challenges for Detectors: In the real world, detecting LLM-generated\n",
      "misinformation is in face with emerging challenges. First, it is difficult to obtain factuality\n",
      "supervision labels to train detectors for LLM-generated misinformation since it is harder\n",
      "for humans to detect than human-written misinformation. Second, malicious users can\n",
      "12/23/24, 2:22 PM\n",
      "LLMs Meet Misinformation\n",
      "https://llm-misinformation.github.io\n",
      "8/15\n",
      "\n",
      "\n",
      "\n",
      "--- Page 9 ---\n",
      "easily utilize methods shown in Table 1 and close-sourced LLMs (e.g., ChatGPT) or open-\n",
      "sourced LLMs (e.g., Llama2 (Touvron et al., 2023b)) to generate misinformation at scale in\n",
      "different domains, types, and errors, which is hard for conventional supervisedly trained\n",
      "detectors to maintain effective. Thus, it is likely to be impractical to apply conventional\n",
      "supervisedly trained detectors to detect LLM-generated misinformation in the practices.\n",
      "Evaluation Setting: We adopt LLMs such as GPT-4 with zero-shot prompting strategies as\n",
      "the representative misinformation detectors to assess and compare the detection\n",
      "hardness of LLMgenerated misinformation and human-written misinformation for two\n",
      "reasons. First, zero-shot setting can better reflect the real-world scenarios of detecting\n",
      "LLM-generated misinformation considering the likely impracticality of conventional\n",
      "supervisedly trained detectors (e.g., BERT) in practices. Second, there are many works\n",
      "that have demonstrated directly prompting LLMs such as GPT-4 in a zero-shot way can\n",
      "outperform conventional supervisedly trained models such as BERT on detecting human-\n",
      "written misinformation (Pelrine et al., 2023; Zhang et al., 2023c; Bang et al., 2023;\n",
      "Buchholz, 2023; Li et al., 2023b), which shows that zero-shot LLMs have already achieved\n",
      "almost state-of-the-art performance in the task of misinformation detection. In the zero-\n",
      "shot setting, we can adopt Success Rate % as the metric to measure the probability of\n",
      "LLM-generated \n",
      "or \n",
      "human-written \n",
      "misinformation \n",
      "being \n",
      "successfully \n",
      "identified,\n",
      "representing the difficulty of being detected.\n",
      "Experiment Result Analysis: First, we can observe that it is also generally hard for LLM\n",
      "detectors to detect ChatGPT-generated misinformation, especially those generated via\n",
      "Hallucinated News Generation, Totally Arbitrary Generation and Open-ended Generation.\n",
      "For example, LLM detectors can hardly detect fine-grained hallucinations.\n",
      "Second, previous works have shown that detectors can perform better than humans on\n",
      "detecting human-written misinformation (Pérez-Rosas et al., 2018). Comparing LLM\n",
      "detection and human detection performance, we can discover that GPT-4 can outperform\n",
      "humans on detecting ChatGPT-generated misinformation, though humans can still\n",
      "perform better than ChatGPT-3.5.\n",
      "12/23/24, 2:22 PM\n",
      "LLMs Meet Misinformation\n",
      "https://llm-misinformation.github.io\n",
      "9/15\n",
      "\n",
      "\n",
      "\n",
      "--- Page 10 ---\n",
      "After evaluating the overall performance of LLM detectors, we aim to further investigate\n",
      "whether or not LLM-generated misinformation can be harder for detectors to detect than\n",
      "human-written misinformation with the same semantics.\n",
      "As shown in Table 4, we can observe that the LLM detection performances on ChatGPT-\n",
      "generated \n",
      "misinformation \n",
      "are \n",
      "mostly \n",
      "lower \n",
      "than \n",
      "those \n",
      "on \n",
      "human-written\n",
      "misinformation. For example, Llama2-7B with \"CoT\" has a performance drop by 19.6% on\n",
      "detecting misinformation generated via Rewriting Generation based on Politifact\n",
      "compared with detecting human-written misinformation. Thus, we can have our third\n",
      "core finding:\n",
      "LLM-generated misinformation can be harder for misinformation detectors to\n",
      "detect\n",
      "than human-written misinformation with the same semantics.\n",
      "Our finding implies that LLM-generated misinformation can have more deceptive styles\n",
      "for detectors and existing detectors are likely to be less effective in detecting LLM-\n",
      "generated misinformation. Also, malicious users could potentially utilize LLMs to escape\n",
      "the detection of detectors.\n",
      "12/23/24, 2:22 PM\n",
      "LLMs Meet Misinformation\n",
      "https://llm-misinformation.github.io\n",
      "10/15\n",
      "\n",
      "\n",
      "\n",
      "--- Page 11 ---\n",
      "Implications on Combating Misinformation\n",
      "at the Age of LLMs\n",
      "Implication 1: our findings directly suggest that humans can be more susceptible to LLM-\n",
      "generated misinformation and detectors can be less effective in detecting LLM-\n",
      "generated misinformation compared with human-written misinformation. In other\n",
      "words, LLM-generated misinformation can be more deceptive and potentially cause\n",
      "more harm.\n",
      "Implication 2: on the one hand, a large amount of hallucinated information is potentially\n",
      "generated by normal users due to the popularity of LLMs. On the other hand, malicious\n",
      "users are more likely to exploit LLMs to generate misinformation to escape the detection\n",
      "of detectors. Thus, there is a potential major paradigm shift of misinformation\n",
      "production from humans to LLMs.\n",
      "Implication 3: considering malicious users can easily prompt LLMs to generate\n",
      "misinformation at scale, which is more deceptive than human-written misinformation,\n",
      "online safety and public trust are faced with serious threats. We call for collective efforts\n",
      "12/23/24, 2:22 PM\n",
      "LLMs Meet Misinformation\n",
      "https://llm-misinformation.github.io\n",
      "11/15\n",
      "\n",
      "\n",
      "\n",
      "--- Page 12 ---\n",
      "on \n",
      "combating \n",
      "LLM-generated \n",
      "misinformation \n",
      "from \n",
      "stakeholders \n",
      "in \n",
      "different\n",
      "backgrounds including researchers, government, platforms, and the general public.\n",
      "Countermeasures Through LLMs’ Lifecycle\n",
      "As shown in Figure 7, we propose to divide the lifecycle of LLMs into three stages and\n",
      "discuss the countermeasures against LLM-generated misinformation through the whole\n",
      "lifecycle. In the training stage, we can curate the training data to remove nonfactual\n",
      "articles and ground the training process to existing knowledge bases (Yu et al., 2020) to\n",
      "reduce LLMs’ hallucinations. Alignment training processes such as RLHF (Casper et al.,\n",
      "2023a) can reduce the risk of generating harmful content. In the Inference stage, we can\n",
      "utilize prompt filtering, intent modeling or jailbreak defenses (Jain et al., 2023) to prevent\n",
      "AMG methods (e.g., Totally Arbitrary Generation), and confidence (or uncertainty)\n",
      "estimation (Xiong et al., 2023) or retrieval augmentation (Mialon et al., 2023) to defend\n",
      "against HG methods (e.g., Hallucinated News Generation). However, they may be\n",
      "ineffective for most of CMG methods (e.g., Rewriting Generation), which are based on\n",
      "human-written misleading content and do not explicitly express the intent of generating\n",
      "misinformation. More research is desired to develop inference-time factually verification\n",
      "methods for combating CMG methods. In the influence stage when LLM-generated\n",
      "content starts to influence the public, it is under-explored how to design effective\n",
      "detectors for LLM-generated misinformation or texts. Also, it is essential to enhance the\n",
      "public’s awareness of LLM-generated misinformation.\n",
      "Ethics Statement\n",
      "Considering that the open-sourced LLMs (e.g., Llama) or close-sourced LLMs (e.g.,\n",
      "ChatGPT) are widely adopted, and the potential approaches to generate misinformation\n",
      "with LLMs are based on real-world scenarios (shown in Table 1) and straightforward to\n",
      "implement, we anticipate these methods have been potentially utilized to generate\n",
      "12/23/24, 2:22 PM\n",
      "LLMs Meet Misinformation\n",
      "https://llm-misinformation.github.io\n",
      "12/15\n",
      "\n",
      "\n",
      "\n",
      "--- Page 13 ---\n",
      "misinformation by normal people unintentionally or malicious users intentionally in the\n",
      "real world. Thus, our research illustrates the landscape of LLM-generated misinformation\n",
      "to shed light on the potential risks, enhance the public’s awareness of its harm, and call\n",
      "for collective countering efforts. We also discuss the implications of our findings and the\n",
      "potential countermeasures, which can inspire and facilitate more future research on\n",
      "defending against LLM-generated misinformation.\n",
      "Examples of ChatGPT-Generated\n",
      "Misinformation\n",
      "PROMPT:\n",
      "You are a journalist. Please write a piece of news.\n",
      "RESPONSE:\n",
      "Headline: New Study Reveals Surprising Benefits of Gardening for Mental Health. \n",
      "Date: October 12, 2022. In a fresh breakthrough highlighting the importance of \n",
      "wellbeing, a landmark study conducted by experts at the renowned Garden Research \n",
      "Institute (GRI) has unearthed remarkable benefits of gardening on mental health. \n",
      "The study, conducted over a span of three years, reveals how nurturing nature can \n",
      "play a pivotal role in combating stress, anxiety, and depression. Gardening \n",
      "enthusiasts \n",
      "and \n",
      "mental \n",
      "health \n",
      "professionals \n",
      "have \n",
      "long \n",
      "acknowledged \n",
      "the \n",
      "therapeutic effects of spending time amid plants and flowers, but this study \n",
      "delves deeper to emphasize the extent of the positive mental health impact. The \n",
      "GRI study included over 2,000 participants from diverse age groups, socio-\n",
      "economic backgrounds, and geographical locations. Researchers used a combination \n",
      "of quantitative surveys and qualitative interviews to gather a comprehensive \n",
      "understanding of the psychological benefits that gardening brings. The findings \n",
      "unveiled an array of advantages that can be attested to gardening. First and \n",
      "foremost, participants reported a significant decrease in stress levels. The act \n",
      "of gardening was shown to encourage relaxation, helping individuals unwind and \n",
      "find solace amidst the natural world. Moreover, the study demonstrated that \n",
      "gardening had the power to stabilize moods, with over 81% of participants \n",
      "experiencing reduced feelings of anxiety and depression. Interestingly, the \n",
      "therapeutic connection between people and plants wasn’t limited to the emotional \n",
      "realm. The GRI study indicated a tangible positive correlation between gardening \n",
      "Example 1 of Hallucinated News Generation method.\n",
      "12/23/24, 2:22 PM\n",
      "LLMs Meet Misinformation\n",
      "https://llm-misinformation.github.io\n",
      "13/15\n",
      "\n",
      "\n",
      "\n",
      "--- Page 14 ---\n",
      "and physical well-being. Physical ailments such as high blood pressure, sleep \n",
      "disorders, and chronic pain were reported with lower frequency among participants \n",
      "who engaged in regular gardening activities. Elaborating on the findings, Dr. \n",
      "Emily Stevens, lead researcher at GRI, emphasized, “Gardening provides a multi-\n",
      "faceted approach to improving mental health. It combines the benefits of physical \n",
      "exercise, connecting with nature, and nurturing living organisms, which \n",
      "ultimately empowers individuals to regain a sense of control and purpose in their \n",
      "lives, leading to improved mental well-being.” The study also highlighted the \n",
      "social aspects associated with gardening, especially within community gardens or \n",
      "allotments. Participants expressed a higher sense of community belonging, reduced \n",
      "feelings of isolation, and increased social interactions. Gardening, it seems, \n",
      "has the potential to bring people together, fostering a stronger sense of unity \n",
      "and shared purpose.\n",
      "Acknowledgement\n",
      "This material is based upon work supported by the U.S. Department of Homeland\n",
      "Security under Grant Award Number 17STQAC00001-07-04, and the Office of the Director\n",
      "of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA),\n",
      "via the HIATUS Program contract #2022-22072200001, NSF SaTC-2241068, a Cisco\n",
      "Research Award, a Microsoft Accelerate Foundation Models Research Award. The views\n",
      "and conclusions contained in this document are those of the authors and should not be\n",
      "interpreted as necessarily representing the official policies, either expressed or implied,\n",
      "of the U.S. Department of Homeland Security, ODNI, IARPA, or the U.S. Government. The\n",
      "U.S. Government is authorized to reproduce and distribute reprints for governmental\n",
      "purposes notwithstanding any copyright annotation therein.\n",
      "BibTeX\n",
      "@inproceedings{chen2024llmgenerated,\n",
      "      title={Can {LLM}-Generated Misinformation Be Detected?},\n",
      "      author={Canyu Chen and Kai Shu},\n",
      "      booktitle={The Twelfth International Conference on Learning Representations\n",
      "      year={2024},\n",
      "12/23/24, 2:22 PM\n",
      "LLMs Meet Misinformation\n",
      "https://llm-misinformation.github.io\n",
      "14/15\n",
      "\n",
      "\n",
      "\n",
      "--- Page 15 ---\n",
      "      url={https://openreview.net/forum?id=ccxD4mtkTU}\n",
      "      }\n",
      "This website is licensed under a Creative Commons\n",
      "Attribution-ShareAlike 4.0 International License.\n",
      "This means you are free to borrow the source code of this\n",
      "website, we just ask that you link back to this page in the\n",
      "footer. Please remember to remove the analytics code\n",
      "included in the header of the website which you do not\n",
      "want on your website.\n",
      "12/23/24, 2:22 PM\n",
      "LLMs Meet Misinformation\n",
      "https://llm-misinformation.github.io\n",
      "15/15\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extract text from each page and print it\n",
    "for page_num in range(num_pages):\n",
    "    page = pdf_document.load_page(page_num)\n",
    "    text = page.get_text()\n",
    "    print(f'--- Page {page_num + 1} ---')\n",
    "    print(text)\n",
    "    print('\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing Text Paragraph Wise\n",
    "\n",
    "### Defining a Function to Extract Paragraphs from Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_by_token_count(text, threshold=200):\n",
    "    # Split text by whitespace to count tokens\n",
    "    tokens = re.findall(r'\\S+', text)\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_token_count = 0\n",
    "\n",
    "    for token in tokens:\n",
    "        if current_token_count + 1 > threshold:\n",
    "            # If adding this token exceeds the threshold, save the current chunk and start a new one\n",
    "            chunks.append(' '.join(current_chunk))\n",
    "            current_chunk = [token]\n",
    "            current_token_count = 1\n",
    "        else:\n",
    "            # Otherwise, continue adding to the current chunk\n",
    "            current_chunk.append(token)\n",
    "            current_token_count += 1\n",
    "\n",
    "    # Append the last chunk if it exists\n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join(current_chunk))\n",
    "\n",
    "    return chunks\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting and Printing Paragraphs from Each Page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Page 1 ---\n",
      "Paragraph 1:\n",
      "This is an initiative aiming to combat misinformation in the age of LLMs (Correspondence to: Kai Shu) (New Preprint) Can Knowledge Editing Really Correct Hallucinations? - We proposed HalluEditBench to holistically benchmark knowledge editing methods in correcting real-world hallucinations on five dimensions including Efficacy, Generalization, Portability, Locality, and Robustness. We find their effectiveness could be far from what their performance on existing datasets suggests, and the performance beyond Efficacy for all methods is generally unsatisfactory. (New Preprint) Can Editing LLMs Inject Harm? - We propose to reformulate knowledge editing as a new type of safety threat for LLMs, namely Editing Attack, and discover its emerging risk of injecting misinformation or bias into LLMs stealthily, indicating the feasibility of disseminating misinformation or bias with LLMs as new channels. (SIGKDD Explorations 2024) Authorship Attribution in the Era of LLMs: Problems, Methodologies, and Challenges - This survey paper systematically categorizes authorship attribution in the era of LLMs into four problems: attributing unknown texts to human authors, detecting LLM-generated texts, identifying specific LLMs or human authors, and classifying texts as human-authored, machine-generated, or co-authored by both, while also highlighting key challenges and open problems. (EMNLP 2024 Findings) Can Large Language Models Identify Authorship? -\n",
      "\n",
      "\n",
      "Paragraph 2:\n",
      "We propose Linguistically Informed Prompting (LIP) strategy, which offers in-context linguistic guidance, to boost LLMs' reasoning capacity for authorship verification and attribution tasks, while also providing natural language explanations. (AI Magazine 2024) Combating Misinformation in the Age of LLMs: Opportunities and Challenges - A survey of the opportunities (can we utilize LLMs to combat misinformation) and challenges (how to combat LLM-generated misinformation) of combating misinformation in the age of LLMs. (Proceedings of ICLR 2024) Can LLM-Generated Misinformation Be 12/23/24, 2:22 PM LLMs Meet Misinformation https://llm-misinformation.github.io 1/15\n",
      "\n",
      "\n",
      "--- Page 2 ---\n",
      "Paragraph 1:\n",
      "Detected? - We discover that LLM-generated misinformation can be harder to detect for humans and detectors compared to human-written misinformation with the same semantics, which suggests it can have more deceptive styles and potentially cause more harm. Combating Misinformation in the Age of LLMs: Opportunities and Challenges Canyu Chen, Kai Shu Illinois Institute of Technology Published at AI Magazine 2024 (Volume 45, Issue 3, Fall 2024), Highlight Article Publication Paper arXiv Talk Slides 1 Slides 2 Paper List 12/23/24, 2:22 PM LLMs Meet Misinformation https://llm-misinformation.github.io 2/15\n",
      "\n",
      "\n",
      "--- Page 3 ---\n",
      "Paragraph 1:\n",
      "Abstract Misinformation such as fake news and rumors is a serious threat to information ecosystems and public trust. The emergence of Large Language Models (LLMs) has great potential to reshape the landscape of combating misinformation. Generally, LLMs can be a double-edged sword in the fight. On the one hand, LLMs bring promising opportunities for combating misinformation due to their profound world knowledge and strong reasoning abilities. Thus, one emergent question is: can we utilize LLMs to combat misinformation? On the other hand, the critical challenge is that LLMs can be easily leveraged to generate deceptive misinformation at scale. Then, another important question is: how to combat LLM-generated misinformation? In this paper, we first systematically review the history of combating misinformation before the advent of LLMs. Then we illustrate the current efforts and present an outlook for these two fundamental questions respectively. The goal of this survey paper is to facilitate the progress of utilizing LLMs for fighting misinformation and call for interdisciplinary efforts from different stakeholders for combating LLM-generated misinformation. BibTeX @article{chen2024combatingmisinformation, author = {Chen, Canyu and Shu, Kai}, title = {Combating misinformation in the age of LLMs: Opportunities and cha journal = {AI Magazine}, year = {2024}, doi =\n",
      "\n",
      "\n",
      "Paragraph 2:\n",
      "{10.1002/aaai.12188}, url = {https://doi.org/10.1002/aaai.12188} } 12/23/24, 2:22 PM LLMs Meet Misinformation https://llm-misinformation.github.io 3/15\n",
      "\n",
      "\n",
      "--- Page 4 ---\n",
      "Paragraph 1:\n",
      "Can LLM-Generated Misinformation Be Detected? Canyu Chen, Kai Shu Illinois Institute of Technology Published at Proceedings of ICLR 2024 Publication Paper arXiv Dataset and Code Talk Slides 1 Slides 2 post post post Abstract The advent of Large Language Models (LLMs) has made a transformative impact. However, the potential that LLMs such as ChatGPT can be exploited to generate misinformation has posed a serious concern to online safety and public trust. A fundamental research question is: will LLM-generated misinformation cause more harm than human-written misinformation? We propose to tackle this question from the perspective of detection difficulty. We first build a taxonomy of LLM-generated misinformation. Then we categorize and validate the potential real-world methods for generating 12/23/24, 2:22 PM LLMs Meet Misinformation https://llm-misinformation.github.io 4/15\n",
      "\n",
      "\n",
      "--- Page 5 ---\n",
      "Paragraph 1:\n",
      "misinformation with LLMs. Then, through extensive empirical investigation, we discover that LLM-generated misinformation can be harder to detect for humans and detectors compared to human- written misinformation with the same semantics, which suggests it can have more deceptive styles and potentially cause more harm. We also discuss the implications of our discovery on combating misinformation in the age of LLMs and the countermeasures. Our Contributions (1) We build a taxonomy by types, domains, sources, intents and errors to systematically characterize LLM-generated misinformation as an emerging and critical research topic. (2) We make the first attempt to categorize and validate the potential real-world methods for generating misinformation with LLMs including Hallucination Generation, Arbitrary Misinformation Generation and Controllable Misinformation Generation methods. (3) We discover that misinformation generated by LLMs can be harder for humans and detectors to detect than human-written misinformation with the same semantic information through extensive investigation, which provides sufficient empirical evidence to demonstrate that LLM-generated misinformation can have more deceptive styles and potentially cause more harm. (4) We discuss the emerging challenges for misinformation detectors (Section 6), important implications of our discovery on combating misinformation in the age of LLMs (Section 7), the countermeasures against LLM-generated misinformation through LLMs’\n",
      "\n",
      "\n",
      "Paragraph 2:\n",
      "whole lifecycle (Section 8). Taxonomy of LLM-Generated Misinformation We propose to taxonomize LLM-generated misinformation from five dimensions including types, domains, sources, intents and errors. In particular, we categorize the sources of LLM-generated misinformation into hallucination, arbitrary generation and controllable generation since there are different potential methods to generate misinformation with LLMs. Also, we divide the intents of generated misinformation into unintentional and intentional generation considering hallucination can potentially occur in any generation process of LLMs and users without malicious intent may also generate texts containing hallucinated information when using LLMs. 12/23/24, 2:22 PM LLMs Meet Misinformation https://llm-misinformation.github.io 5/15\n",
      "\n",
      "\n",
      "--- Page 6 ---\n",
      "Paragraph 1:\n",
      "RQ1: How Can LLMs be Utilized to Generate Misinformation? We propose to categorize the LLM-based misinformation generation methods into three types based on real-world scenarios (Table 1): Hallucination Generation (HG), Arbitrary Misinformation Generation (AMG) and Controllable Misinformation Generation (CMG). 12/23/24, 2:22 PM LLMs Meet Misinformation https://llm-misinformation.github.io 6/15\n",
      "\n",
      "\n",
      "--- Page 7 ---\n",
      "Paragraph 1:\n",
      "Connection with Jailbreak Attack: Jailbreak attacks usually refer to the attempts to bypass the safety guards of LLMs (e.g., ChatGPT) to generate harmful content. On the one hand, our proposed approaches to generate misinformation with LLMs are motivated by real- world scenarios shown in Table 1 and orthogonal to the previous Jailbreak techniques (Wei et al., 2023a; Zou et al., 2023), which suggests the misinformation generation approaches and previous jailbreak methods could be potentially combined by attackers. On the other hand, the HG methods could be regarded as Unintentional Jailbreak, which is different from most previous jailbreak methods. The AMG and CMG methods could be regarded as Intentional Jailbreak. We test the possibilities that our misinformation generation approaches can bypass the safeguard of ChatGPT by prompting with each approach for 100 times. The Attacking Success Rates are in Table 2. Thus, our first core finding is: LLMs can follow users’ instructions to generate misinformation in different types, domains, and errors. RQ2: Can Humans Detect LLM-Generated Misinformation? Although previous works have shown that it is hard for humans to detect human-written misinformation (Lyons et al., 2021), it is still under-explored whether or not humans can detect LLM-generated misinformation. Experiment Result Analysis:\n",
      "\n",
      "\n",
      "Paragraph 2:\n",
      "First, we can observe that it is generally hard for humans to detect ChatGPT-generated misinformation, especially those generated with Hallucinated News Generation, Totally Arbitrary Generation, Rewriting Generation, and Open-ended Generation. 12/23/24, 2:22 PM LLMs Meet Misinformation https://llm-misinformation.github.io 7/15\n",
      "\n",
      "\n",
      "--- Page 8 ---\n",
      "Paragraph 1:\n",
      "Second, we attempt to compare the human detection’s hardness for ChatGPT-generated and human-written misinformation that have the same semantics. We have demonstrated that Paraphrase Generation, Rewriting Generation, and Open-ended Generation generally only change the style information and preserve the original semantics. Comparing human detection performance on human-written misinformation (the grey numbers in Table 3) and ChatGPT-generated misinformation via Paraphrase Generation, Rewriting Generation and Open-ended Generation approaches (the red or green numbers in Table 3), we can discover that the human detection performances on ChatGPT-generated misinformation are mostly lower than those on human-written misinformation. Thus, we can have our second core finding shown as follows: LLM-generated misinformation can be harder for humans to detect than human-written misinformation with the same semantics. Our finding also validates that LLM-generated misinformation can have more deceptive styles for humans and implies humans can be potentially more susceptible to LLM- generated misinformation than human-written misinformation. RQ3: Can Detectors Detect LLM-Generated Misinformation? Emerging Challenges for Detectors: In the real world, detecting LLM-generated misinformation is in face with emerging challenges. First, it is difficult to obtain factuality supervision labels to train detectors for LLM-generated misinformation since it is harder for humans to detect than human-written misinformation. Second, malicious\n",
      "\n",
      "\n",
      "Paragraph 2:\n",
      "users can 12/23/24, 2:22 PM LLMs Meet Misinformation https://llm-misinformation.github.io 8/15\n",
      "\n",
      "\n",
      "--- Page 9 ---\n",
      "Paragraph 1:\n",
      "easily utilize methods shown in Table 1 and close-sourced LLMs (e.g., ChatGPT) or open- sourced LLMs (e.g., Llama2 (Touvron et al., 2023b)) to generate misinformation at scale in different domains, types, and errors, which is hard for conventional supervisedly trained detectors to maintain effective. Thus, it is likely to be impractical to apply conventional supervisedly trained detectors to detect LLM-generated misinformation in the practices. Evaluation Setting: We adopt LLMs such as GPT-4 with zero-shot prompting strategies as the representative misinformation detectors to assess and compare the detection hardness of LLMgenerated misinformation and human-written misinformation for two reasons. First, zero-shot setting can better reflect the real-world scenarios of detecting LLM-generated misinformation considering the likely impracticality of conventional supervisedly trained detectors (e.g., BERT) in practices. Second, there are many works that have demonstrated directly prompting LLMs such as GPT-4 in a zero-shot way can outperform conventional supervisedly trained models such as BERT on detecting human- written misinformation (Pelrine et al., 2023; Zhang et al., 2023c; Bang et al., 2023; Buchholz, 2023; Li et al., 2023b), which shows that zero-shot LLMs have already achieved almost state-of-the-art performance in the task of misinformation detection. In the zero- shot setting, we can adopt Success Rate\n",
      "\n",
      "\n",
      "Paragraph 2:\n",
      "% as the metric to measure the probability of LLM-generated or human-written misinformation being successfully identified, representing the difficulty of being detected. Experiment Result Analysis: First, we can observe that it is also generally hard for LLM detectors to detect ChatGPT-generated misinformation, especially those generated via Hallucinated News Generation, Totally Arbitrary Generation and Open-ended Generation. For example, LLM detectors can hardly detect fine-grained hallucinations. Second, previous works have shown that detectors can perform better than humans on detecting human-written misinformation (Pérez-Rosas et al., 2018). Comparing LLM detection and human detection performance, we can discover that GPT-4 can outperform humans on detecting ChatGPT-generated misinformation, though humans can still perform better than ChatGPT-3.5. 12/23/24, 2:22 PM LLMs Meet Misinformation https://llm-misinformation.github.io 9/15\n",
      "\n",
      "\n",
      "--- Page 10 ---\n",
      "Paragraph 1:\n",
      "After evaluating the overall performance of LLM detectors, we aim to further investigate whether or not LLM-generated misinformation can be harder for detectors to detect than human-written misinformation with the same semantics. As shown in Table 4, we can observe that the LLM detection performances on ChatGPT- generated misinformation are mostly lower than those on human-written misinformation. For example, Llama2-7B with \"CoT\" has a performance drop by 19.6% on detecting misinformation generated via Rewriting Generation based on Politifact compared with detecting human-written misinformation. Thus, we can have our third core finding: LLM-generated misinformation can be harder for misinformation detectors to detect than human-written misinformation with the same semantics. Our finding implies that LLM-generated misinformation can have more deceptive styles for detectors and existing detectors are likely to be less effective in detecting LLM- generated misinformation. Also, malicious users could potentially utilize LLMs to escape the detection of detectors. 12/23/24, 2:22 PM LLMs Meet Misinformation https://llm-misinformation.github.io 10/15\n",
      "\n",
      "\n",
      "--- Page 11 ---\n",
      "Paragraph 1:\n",
      "Implications on Combating Misinformation at the Age of LLMs Implication 1: our findings directly suggest that humans can be more susceptible to LLM- generated misinformation and detectors can be less effective in detecting LLM- generated misinformation compared with human-written misinformation. In other words, LLM-generated misinformation can be more deceptive and potentially cause more harm. Implication 2: on the one hand, a large amount of hallucinated information is potentially generated by normal users due to the popularity of LLMs. On the other hand, malicious users are more likely to exploit LLMs to generate misinformation to escape the detection of detectors. Thus, there is a potential major paradigm shift of misinformation production from humans to LLMs. Implication 3: considering malicious users can easily prompt LLMs to generate misinformation at scale, which is more deceptive than human-written misinformation, online safety and public trust are faced with serious threats. We call for collective efforts 12/23/24, 2:22 PM LLMs Meet Misinformation https://llm-misinformation.github.io 11/15\n",
      "\n",
      "\n",
      "--- Page 12 ---\n",
      "Paragraph 1:\n",
      "on combating LLM-generated misinformation from stakeholders in different backgrounds including researchers, government, platforms, and the general public. Countermeasures Through LLMs’ Lifecycle As shown in Figure 7, we propose to divide the lifecycle of LLMs into three stages and discuss the countermeasures against LLM-generated misinformation through the whole lifecycle. In the training stage, we can curate the training data to remove nonfactual articles and ground the training process to existing knowledge bases (Yu et al., 2020) to reduce LLMs’ hallucinations. Alignment training processes such as RLHF (Casper et al., 2023a) can reduce the risk of generating harmful content. In the Inference stage, we can utilize prompt filtering, intent modeling or jailbreak defenses (Jain et al., 2023) to prevent AMG methods (e.g., Totally Arbitrary Generation), and confidence (or uncertainty) estimation (Xiong et al., 2023) or retrieval augmentation (Mialon et al., 2023) to defend against HG methods (e.g., Hallucinated News Generation). However, they may be ineffective for most of CMG methods (e.g., Rewriting Generation), which are based on human-written misleading content and do not explicitly express the intent of generating misinformation. More research is desired to develop inference-time factually verification methods for combating CMG methods. In the influence stage when LLM-generated content starts\n",
      "\n",
      "\n",
      "Paragraph 2:\n",
      "to influence the public, it is under-explored how to design effective detectors for LLM-generated misinformation or texts. Also, it is essential to enhance the public’s awareness of LLM-generated misinformation. Ethics Statement Considering that the open-sourced LLMs (e.g., Llama) or close-sourced LLMs (e.g., ChatGPT) are widely adopted, and the potential approaches to generate misinformation with LLMs are based on real-world scenarios (shown in Table 1) and straightforward to implement, we anticipate these methods have been potentially utilized to generate 12/23/24, 2:22 PM LLMs Meet Misinformation https://llm-misinformation.github.io 12/15\n",
      "\n",
      "\n",
      "--- Page 13 ---\n",
      "Paragraph 1:\n",
      "misinformation by normal people unintentionally or malicious users intentionally in the real world. Thus, our research illustrates the landscape of LLM-generated misinformation to shed light on the potential risks, enhance the public’s awareness of its harm, and call for collective countering efforts. We also discuss the implications of our findings and the potential countermeasures, which can inspire and facilitate more future research on defending against LLM-generated misinformation. Examples of ChatGPT-Generated Misinformation PROMPT: You are a journalist. Please write a piece of news. RESPONSE: Headline: New Study Reveals Surprising Benefits of Gardening for Mental Health. Date: October 12, 2022. In a fresh breakthrough highlighting the importance of wellbeing, a landmark study conducted by experts at the renowned Garden Research Institute (GRI) has unearthed remarkable benefits of gardening on mental health. The study, conducted over a span of three years, reveals how nurturing nature can play a pivotal role in combating stress, anxiety, and depression. Gardening enthusiasts and mental health professionals have long acknowledged the therapeutic effects of spending time amid plants and flowers, but this study delves deeper to emphasize the extent of the positive mental health impact. The GRI study included over 2,000 participants from diverse age groups, socio- economic\n",
      "\n",
      "\n",
      "Paragraph 2:\n",
      "backgrounds, and geographical locations. Researchers used a combination of quantitative surveys and qualitative interviews to gather a comprehensive understanding of the psychological benefits that gardening brings. The findings unveiled an array of advantages that can be attested to gardening. First and foremost, participants reported a significant decrease in stress levels. The act of gardening was shown to encourage relaxation, helping individuals unwind and find solace amidst the natural world. Moreover, the study demonstrated that gardening had the power to stabilize moods, with over 81% of participants experiencing reduced feelings of anxiety and depression. Interestingly, the therapeutic connection between people and plants wasn’t limited to the emotional realm. The GRI study indicated a tangible positive correlation between gardening Example 1 of Hallucinated News Generation method. 12/23/24, 2:22 PM LLMs Meet Misinformation https://llm-misinformation.github.io 13/15\n",
      "\n",
      "\n",
      "--- Page 14 ---\n",
      "Paragraph 1:\n",
      "and physical well-being. Physical ailments such as high blood pressure, sleep disorders, and chronic pain were reported with lower frequency among participants who engaged in regular gardening activities. Elaborating on the findings, Dr. Emily Stevens, lead researcher at GRI, emphasized, “Gardening provides a multi- faceted approach to improving mental health. It combines the benefits of physical exercise, connecting with nature, and nurturing living organisms, which ultimately empowers individuals to regain a sense of control and purpose in their lives, leading to improved mental well-being.” The study also highlighted the social aspects associated with gardening, especially within community gardens or allotments. Participants expressed a higher sense of community belonging, reduced feelings of isolation, and increased social interactions. Gardening, it seems, has the potential to bring people together, fostering a stronger sense of unity and shared purpose. Acknowledgement This material is based upon work supported by the U.S. Department of Homeland Security under Grant Award Number 17STQAC00001-07-04, and the Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), via the HIATUS Program contract #2022-22072200001, NSF SaTC-2241068, a Cisco Research Award, a Microsoft Accelerate Foundation Models Research Award. The views and conclusions contained in this document are those\n",
      "\n",
      "\n",
      "Paragraph 2:\n",
      "of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of the U.S. Department of Homeland Security, ODNI, IARPA, or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for governmental purposes notwithstanding any copyright annotation therein. BibTeX @inproceedings{chen2024llmgenerated, title={Can {LLM}-Generated Misinformation Be Detected?}, author={Canyu Chen and Kai Shu}, booktitle={The Twelfth International Conference on Learning Representations year={2024}, 12/23/24, 2:22 PM LLMs Meet Misinformation https://llm-misinformation.github.io 14/15\n",
      "\n",
      "\n",
      "--- Page 15 ---\n",
      "Paragraph 1:\n",
      "url={https://openreview.net/forum?id=ccxD4mtkTU} } This website is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License. This means you are free to borrow the source code of this website, we just ask that you link back to this page in the footer. Please remember to remove the analytics code included in the header of the website which you do not want on your website. 12/23/24, 2:22 PM LLMs Meet Misinformation https://llm-misinformation.github.io 15/15\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for page_num in range(num_pages):\n",
    "    page = pdf_document.load_page(page_num)\n",
    "    text = page.get_text()\n",
    "    paragraphs = extract_text_by_token_count(text)\n",
    "    print(f'--- Page {page_num + 1} ---')\n",
    "    for para_num, paragraph in enumerate(paragraphs, start=1):\n",
    "        print(f'Paragraph {para_num}:')\n",
    "        print(paragraph)\n",
    "        print('\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Parsing and Analysis\n",
    "#### Saving Extracted Text to a File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save all chunks to a single text file, separating them with page numbers and chunk numbers\n",
    "extracted_data = []\n",
    "\n",
    "for page_num in range(num_pages):\n",
    "    page = pdf_document.load_page(page_num)\n",
    "    text = page.get_text()\n",
    "    chunks = extract_text_by_token_count(text)\n",
    "    page_data = {\n",
    "        'page_number': page_num + 1,\n",
    "        'chunks': [{'paragraph': chunk_num + 1, 'text': chunk} for chunk_num, chunk in enumerate(chunks)]\n",
    "    }\n",
    "    extracted_data.append(page_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the extracted data to a JSON file\n",
    "with open('extracted_data.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(extracted_data, f, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ihackenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
